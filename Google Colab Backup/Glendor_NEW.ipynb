{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RwO5ZT8xN9_x",
        "VsN4qKU4hYNB",
        "wDdiCwZlEu4f",
        "jHO5JG7lUOKh",
        "_BcowiPhYsAl",
        "n4PyHIB1Hbn7",
        "mycjv9cYxa4i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9d3441acdd84f39b6ec84faea9baea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_460c28bcf25047f6b8611af46c1f211e",
              "IPY_MODEL_861f1aec4b07447d996fda1f2167eaaa",
              "IPY_MODEL_216e4b5360c044fdb0f8d25594aefb88"
            ],
            "layout": "IPY_MODEL_22907696c008464ab0c6cd55078de0d4"
          }
        },
        "460c28bcf25047f6b8611af46c1f211e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa83461caec74382ae23d86ed1747afc",
            "placeholder": "​",
            "style": "IPY_MODEL_df68b7e766d747a4b9669543f17cc263",
            "value": "Creating video: 100%"
          }
        },
        "861f1aec4b07447d996fda1f2167eaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4abccb827c04adb9e7fe80dc1dee61d",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0835b66d3df34cb983800b12a2b652ec",
            "value": 122
          }
        },
        "216e4b5360c044fdb0f8d25594aefb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9240fdf4350e43caaf7d14d0f26717dd",
            "placeholder": "​",
            "style": "IPY_MODEL_bf667d9e931241449e16f308a175acb7",
            "value": " 122/122 [00:01&lt;00:00, 68.83it/s]"
          }
        },
        "22907696c008464ab0c6cd55078de0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa83461caec74382ae23d86ed1747afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df68b7e766d747a4b9669543f17cc263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4abccb827c04adb9e7fe80dc1dee61d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0835b66d3df34cb983800b12a2b652ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9240fdf4350e43caaf7d14d0f26717dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf667d9e931241449e16f308a175acb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **STITCHING FRAMES TO VIDEO**"
      ],
      "metadata": {
        "id": "bX1kszPkCELV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "tSTYkHfUBwW5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Remove all image files from /content if you uploaded them by accident\n",
        "# !rm /content/[0-9][0-9][0-9][0-9][0-9][0-9].png"
      ],
      "metadata": {
        "id": "GotEVn76HiVt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def natural_sort_key(s):\n",
        "    \"\"\"\n",
        "    Helper function for sorting filenames in format '000000.png', '000001.jpg', etc.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Split the filename from extension and convert numeric part to integer\n",
        "        filename_without_ext = s.rsplit('.', 1)[0]\n",
        "        return int(filename_without_ext)\n",
        "    except (ValueError, IndexError):\n",
        "        print(f\"Warning: Unexpected filename format: {s}\")\n",
        "        return s\n",
        "\n",
        "\n",
        "def create_video_from_images(image_folder, output_path, fps=30, size=None):\n",
        "    \"\"\"\n",
        "    Convert a sequence of images into a video file.\n",
        "\n",
        "    Parameters:\n",
        "    - image_folder: Path to the folder containing image sequences\n",
        "    - output_path: Path where the output video will be saved\n",
        "    - fps: Frames per second for the output video (default: 30)\n",
        "    - size: Tuple of (width, height) for output video. If None, uses first image's size\n",
        "    \"\"\"\n",
        "    # Get list of image files and sort them naturally\n",
        "    image_files = [f for f in os.listdir(image_folder)\n",
        "                  if f.rsplit('.', 1)[0].isdigit() and  # Check if filename without extension is numeric\n",
        "                  len(f.rsplit('.', 1)[0]) == 6 and  # Check if numeric part is 6 digits\n",
        "                  f.lower().endswith(('.png', '.jpg', '.jpeg'))]  # Check for image extensions\n",
        "    if not image_files:\n",
        "        print(f\"No images found in {image_folder}\")\n",
        "        return False\n",
        "\n",
        "    # Sort files naturally\n",
        "    image_files.sort(key=natural_sort_key)\n",
        "    print(f\"Found {len(image_files)} images\")\n",
        "\n",
        "    # Read the first image to get dimensions if size is not specified\n",
        "    first_image_path = os.path.join(image_folder, image_files[0])\n",
        "    first_image = cv2.imread(first_image_path)\n",
        "    if first_image is None:\n",
        "        print(f\"Could not read first image: {first_image_path}\")\n",
        "        return False\n",
        "\n",
        "    if size is None:\n",
        "        height, width = first_image.shape[:2]\n",
        "        size = (width, height)\n",
        "        print(f\"Video dimensions will be {width}x{height}\")\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, size)\n",
        "\n",
        "    try:\n",
        "        # Process each image with tqdm progress bar\n",
        "        for filename in tqdm(image_files, desc=\"Creating video\"):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            frame = cv2.imread(image_path)\n",
        "\n",
        "            if frame is None:\n",
        "                print(f\"\\nCould not read image: {image_path}\")\n",
        "                continue\n",
        "\n",
        "            # Resize frame if necessary\n",
        "            if frame.shape[:2] != size[::-1]:\n",
        "                frame = cv2.resize(frame, size)\n",
        "\n",
        "            # Write the frame\n",
        "            out.write(frame)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError occurred: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        # Release everything\n",
        "        out.release()\n",
        "        print(\"\\nVideo creation completed!\")\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "Nz3iRP-dBqS6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# 1. Update this path to your images folder in Colab\n",
        "IMAGE_FOLDER = \"/content/imgs\"\n",
        "\n",
        "# 2. Set output path in Colab's temporary storage\n",
        "OUTPUT_VIDEO = \"/content/output_video.mp4\"\n",
        "\n",
        "# 3. Set desired FPS\n",
        "FPS = 2\n",
        "\n",
        "# Create the video\n",
        "success = create_video_from_images(IMAGE_FOLDER, OUTPUT_VIDEO, FPS)\n",
        "\n",
        "if success:\n",
        "    print(f\"You can now manually download the video from {OUTPUT_VIDEO}\")\n",
        "else:\n",
        "    print(\"Video creation failed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "d9d3441acdd84f39b6ec84faea9baea2",
            "460c28bcf25047f6b8611af46c1f211e",
            "861f1aec4b07447d996fda1f2167eaaa",
            "216e4b5360c044fdb0f8d25594aefb88",
            "22907696c008464ab0c6cd55078de0d4",
            "aa83461caec74382ae23d86ed1747afc",
            "df68b7e766d747a4b9669543f17cc263",
            "f4abccb827c04adb9e7fe80dc1dee61d",
            "0835b66d3df34cb983800b12a2b652ec",
            "9240fdf4350e43caaf7d14d0f26717dd",
            "bf667d9e931241449e16f308a175acb7"
          ]
        },
        "id": "FytVT_5oCOT4",
        "outputId": "00c07889-34fe-4465-9725-1418bb9d38b3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 122 images\n",
            "Video dimensions will be 640x480\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating video:   0%|          | 0/122 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9d3441acdd84f39b6ec84faea9baea2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Video creation completed!\n",
            "You can now manually download the video from /content/output_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixing dependency issues (for insightface or onnxruntime, can't remember) - *Might not need this*"
      ],
      "metadata": {
        "id": "RwO5ZT8xN9_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: This command reports the maximum CUDA version that your GPU driver supports, which in this case is 12.4.\n",
        "# This indicates my GPU can potentially run applications compiled with CUDA versions up to 12.4.\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV0YPKxvLWNh",
        "outputId": "f51b5b5d-3653-4ca1-aaa0-574ca1f68869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxOyzOWdMb0F",
        "outputId": "8d1d60e6-322a-4d3f-959f-8f078cc6f12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.1.2\n",
            "Uninstalling torch-2.1.2:\n",
            "  Successfully uninstalled torch-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.5.0+cu124 torchvision==0.20.0+cu124 torchaudio==2.5.0+cu124 --extra-index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKQI731SMdtm",
        "outputId": "043644ad-ee11-4ba3-f9ed-ce92f0adc0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.5.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.5.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.20.0+cu124 in /usr/local/lib/python3.11/dist-packages (0.20.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.5.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.5.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.0+cu124) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.0+cu124) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.0+cu124) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MAIN STARTING POINT**"
      ],
      "metadata": {
        "id": "f4XBdzGVXouk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe mtcnn retina-face==0.0.12 keras==2.13.1 ultralytics insightface onnxruntime dlib opencv-python opencv-contrib-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ogmV3lURQ1Sj",
        "outputId": "54a8cda6-0852-4a0d-f2eb-d209721515df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: retina-face==0.0.12 in /usr/local/lib/python3.11/dist-packages (0.0.12)\n",
            "Requirement already satisfied: keras==2.13.1 in /usr/local/lib/python3.11/dist-packages (2.13.1)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.72)\n",
            "Requirement already satisfied: insightface in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.20.1)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from retina-face==0.0.12) (1.24.3)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from retina-face==0.0.12) (5.2.0)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from retina-face==0.0.12) (11.1.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from retina-face==0.0.12) (2.13.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (1.4.2)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (4.4.3)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.1.2)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.16.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from insightface) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from insightface) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from insightface) (0.25.1)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from insightface) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from insightface) (3.0.11)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from insightface) (1.3.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from insightface) (3.14.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->retina-face==0.0.12) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->retina-face==0.0.12) (3.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (1.70.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (1.17.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->retina-face==0.0.12) (0.37.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.82)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (4.11.0.86)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (0.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->insightface) (0.2.13)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face==0.0.12) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (3.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face==0.0.12) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face==0.0.12) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=1.9.0->retina-face==0.0.12) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from mtcnn import MTCNN\n",
        "from retinaface import RetinaFace\n",
        "from ultralytics import YOLO\n",
        "import insightface\n",
        "import dlib\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7XG3JFWQQ5xS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For CenterFace **(NOT USING ANYMORE BECAUSE OF ISSUES)**"
      ],
      "metadata": {
        "id": "VsN4qKU4hYNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Gotta install from GH\n",
        "# !git clone https://github.com/Star-Clouds/CenterFace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6n8f6wrhW8l",
        "outputId": "c6d640fd-4146-4ad4-fa09-c332dd146a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CenterFace' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sys.path.append('CenterFace/prj-python')"
      ],
      "metadata": {
        "id": "X9H1al7d60Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download model weights\n",
        "# !wget https://raw.githubusercontent.com/Star-Clouds/CenterFace/master/models/onnx/centerface.onnx\n",
        "# !wget https://raw.githubusercontent.com/Star-Clouds/CenterFace/master/models/onnx/centerface_bnmerged.onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH85vQZFhins",
        "outputId": "8405f427-29c9-41cf-b291-19e0f85ed850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-30 20:35:10--  https://raw.githubusercontent.com/Star-Clouds/CenterFace/master/models/onnx/centerface.onnx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7532772 (7.2M) [application/octet-stream]\n",
            "Saving to: ‘centerface.onnx’\n",
            "\n",
            "centerface.onnx     100%[===================>]   7.18M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-01-30 20:35:11 (129 MB/s) - ‘centerface.onnx’ saved [7532772/7532772]\n",
            "\n",
            "--2025-01-30 20:35:11--  https://raw.githubusercontent.com/Star-Clouds/CenterFace/master/models/onnx/centerface_bnmerged.onnx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7304518 (7.0M) [application/octet-stream]\n",
            "Saving to: ‘centerface_bnmerged.onnx’\n",
            "\n",
            "centerface_bnmerged 100%[===================>]   6.97M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-01-30 20:35:11 (154 MB/s) - ‘centerface_bnmerged.onnx’ saved [7304518/7304518]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from centerface import CenterFace"
      ],
      "metadata": {
        "id": "wsJrDOCq8o90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Modify centerface.py content\n",
        "\n",
        "# # Read the file\n",
        "# with open('CenterFace/prj-python/centerface.py', 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Replace these lines in the __init__ function:\n",
        "# old_init = \"\"\"        if self.landmarks:\n",
        "#             self.net = cv2.dnn.readNetFromONNX('../models/onnx/centerface.onnx')\n",
        "#         else:\n",
        "#             self.net = cv2.dnn.readNetFromONNX('../models/onnx/cface.1k.onnx')\"\"\"\n",
        "\n",
        "# new_init = \"\"\"        if self.landmarks:\n",
        "#             self.net = cv2.dnn.readNetFromONNX('centerface.onnx')\n",
        "#         else:\n",
        "#             self.net = cv2.dnn.readNetFromONNX('centerface_bnmerged.onnx')\"\"\"\n",
        "\n",
        "# new_content = content.replace(old_init, new_init)\n",
        "\n",
        "# # Write the modified content back\n",
        "# with open('CenterFace/prj-python/centerface.py', 'w') as file:\n",
        "#     file.write(new_content)"
      ],
      "metadata": {
        "id": "E5j_Xw6prdEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read and modify centerface.py\n",
        "# with open('CenterFace/prj-python/centerface.py', 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Replace the forward calls with correct layer names\n",
        "# content = content.replace(\n",
        "#     'heatmap, scale, offset = self.net.forward([\"535\", \"536\", \"537\"])',\n",
        "#     'heatmap, scale, offset = self.net.forward([\"onnx_node_output_0!535\", \"onnx_node_output_0!536\", \"onnx_node_output_0!537\"])'\n",
        "# )\n",
        "\n",
        "# content = content.replace(\n",
        "#     'heatmap, scale, offset, lms = self.net.forward([\"537\", \"538\", \"539\", \"540\"])',\n",
        "#     'heatmap, scale, offset, lms = self.net.forward([\"onnx_node_output_0!537\", \"onnx_node_output_0!538\", \"onnx_node_output_0!539\", \"onnx_node_output_0!540\"])'\n",
        "# )\n",
        "\n",
        "# # Write back the modified content\n",
        "# with open('CenterFace/prj-python/centerface.py', 'w') as file:\n",
        "#     file.write(content)"
      ],
      "metadata": {
        "id": "FVCu3EgIvpMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read and modify centerface.py\n",
        "# # with open('CenterFace/prj-python/centerface.py', 'r') as file:\n",
        "# #     content = file.read()\n",
        "\n",
        "# new_content = \"\"\"import numpy as np\n",
        "# import cv2\n",
        "# import datetime\n",
        "\n",
        "\n",
        "# class CenterFace(object):\n",
        "#     def __init__(self, landmarks=True):\n",
        "#         self.landmarks = landmarks\n",
        "#         if self.landmarks:\n",
        "#             self.net = cv2.dnn.readNetFromONNX('centerface.onnx')\n",
        "#         else:\n",
        "#             self.net = cv2.dnn.readNetFromONNX('centerface_bnmerged.onnx')\n",
        "#         self.img_h_new, self.img_w_new, self.scale_h, self.scale_w = 0, 0, 0, 0\n",
        "\n",
        "#     def __call__(self, img, height, width, threshold=0.5):\n",
        "#         self.img_h_new, self.img_w_new, self.scale_h, self.scale_w = self.transform(height, width)\n",
        "#         return self.inference_opencv(img, threshold)\n",
        "\n",
        "#     def inference_opencv(self, img, threshold):\n",
        "#         blob = cv2.dnn.blobFromImage(img, scalefactor=1.0, size=(self.img_w_new, self.img_h_new), mean=(0, 0, 0), swapRB=True, crop=False)\n",
        "#         self.net.setInput(blob)\n",
        "#         begin = datetime.datetime.now()\n",
        "#         lms = None  # Initialize lms\n",
        "#         if self.landmarks:\n",
        "#             heatmap, scale, offset, lms = self.net.forward([\"onnx_node_output_0!537\", \"onnx_node_output_0!538\", \"onnx_node_output_0!539\", \"onnx_node_output_0!540\"])\n",
        "#         else:\n",
        "#             heatmap, scale, offset = self.net.forward([\"onnx_node_output_0!535\", \"onnx_node_output_0!536\", \"onnx_node_output_0!537\"])\n",
        "#         end = datetime.datetime.now()\n",
        "#         print(\"cpu times = \", end - begin)\n",
        "#         return self.postprocess(heatmap, lms, offset, scale, threshold)\"\"\"\n",
        "\n",
        "# # Write the entire new content\n",
        "# with open('CenterFace/prj-python/centerface.py', 'w') as file:\n",
        "#     file.write(new_content + \"\"\"\n",
        "#     def transform(self, h, w):\n",
        "#         img_h_new, img_w_new = int(np.ceil(h / 32) * 32), int(np.ceil(w / 32) * 32)\n",
        "#         scale_h, scale_w = img_h_new / h, img_w_new / w\n",
        "#         return img_h_new, img_w_new, scale_h, scale_w\n",
        "\n",
        "#     def postprocess(self, heatmap, lms, offset, scale, threshold):\n",
        "#         if self.landmarks:\n",
        "#             dets, lms = self.decode(heatmap, scale, offset, lms, (self.img_h_new, self.img_w_new), threshold=threshold)\n",
        "#         else:\n",
        "#             dets = self.decode(heatmap, scale, offset, None, (self.img_h_new, self.img_w_new), threshold=threshold)\n",
        "#         if len(dets) > 0:\n",
        "#             dets[:, 0:4:2], dets[:, 1:4:2] = dets[:, 0:4:2] / self.scale_w, dets[:, 1:4:2] / self.scale_h\n",
        "#             if self.landmarks:\n",
        "#                 lms[:, 0:10:2], lms[:, 1:10:2] = lms[:, 0:10:2] / self.scale_w, lms[:, 1:10:2] / self.scale_h\n",
        "#         else:\n",
        "#             dets = np.empty(shape=[0, 5], dtype=np.float32)\n",
        "#             if self.landmarks:\n",
        "#                 lms = np.empty(shape=[0, 10], dtype=np.float32)\n",
        "#         if self.landmarks:\n",
        "#             return dets, lms\n",
        "#         else:\n",
        "#             return dets, None\n",
        "\n",
        "#     def decode(self, heatmap, scale, offset, landmark, size, threshold=0.1):  # Increased threshold\n",
        "#         heatmap = np.squeeze(heatmap)  # Remove batch dimension\n",
        "#         # We need to handle the 3D heatmap properly\n",
        "#         heatmap = heatmap[0]  # Take first channel\n",
        "#         scale = np.squeeze(scale)\n",
        "#         offset = np.squeeze(offset)\n",
        "\n",
        "#         # Find top k detections instead of all points above threshold\n",
        "#         max_detections = 1000  # Limit number of detections\n",
        "#         heatmap_flat = heatmap.flatten()\n",
        "#         top_k_indices = np.argsort(heatmap_flat)[-max_detections:]\n",
        "#         top_k_values = heatmap_flat[top_k_indices]\n",
        "\n",
        "#         # Convert flat indices back to 2D coordinates\n",
        "#         h, w = heatmap.shape\n",
        "#         c0 = top_k_indices // w\n",
        "#         c1 = top_k_indices % w\n",
        "\n",
        "#         if self.landmarks:\n",
        "#             boxes, lms = [], []\n",
        "#         else:\n",
        "#             boxes = []\n",
        "\n",
        "#         if len(c0) > 0:\n",
        "#             for i in range(len(c0)):\n",
        "#                 s0, s1 = np.exp(scale[c0[i], c1[i]]) * 4, np.exp(scale[c0[i], c1[i]]) * 4\n",
        "#                 o0, o1 = offset[c0[i], c1[i]], offset[c0[i], c1[i]]\n",
        "#                 s = heatmap[c0[i], c1[i]]\n",
        "\n",
        "#                 if s < threshold:  # Skip low confidence detections\n",
        "#                     continue\n",
        "\n",
        "#                 x1, y1 = max(0, (c1[i] + o1 + 0.5) * 4 - s1 / 2), max(0, (c0[i] + o0 + 0.5) * 4 - s0 / 2)\n",
        "#                 x1, y1 = min(x1, size[1]), min(y1, size[0])\n",
        "#                 boxes.append([float(x1), float(y1),\n",
        "#                             float(min(x1 + s1, size[1])),\n",
        "#                             float(min(y1 + s0, size[0])),\n",
        "#                             float(s)])\n",
        "\n",
        "#         if len(boxes) > 0:\n",
        "#             boxes = np.array(boxes, dtype=np.float32)\n",
        "#             keep = self.nms(boxes[:, :4], boxes[:, 4], 0.3)\n",
        "#             boxes = boxes[keep, :]\n",
        "#         else:\n",
        "#             boxes = np.empty((0, 5), dtype=np.float32)\n",
        "\n",
        "#         return boxes\n",
        "\n",
        "#     def nms(self, boxes, scores, nms_thresh):\n",
        "#         x1 = boxes[:, 0]\n",
        "#         y1 = boxes[:, 1]\n",
        "#         x2 = boxes[:, 2]\n",
        "#         y2 = boxes[:, 3]\n",
        "#         areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "#         order = np.argsort(scores)[::-1]\n",
        "#         num_detections = boxes.shape[0]\n",
        "#         suppressed = np.zeros((num_detections,), dtype=np.bool)\n",
        "\n",
        "#         keep = []\n",
        "#         for _i in range(num_detections):\n",
        "#             i = order[_i]\n",
        "#             if suppressed[i]:\n",
        "#                 continue\n",
        "#             keep.append(i)\n",
        "\n",
        "#             ix1 = x1[i]\n",
        "#             iy1 = y1[i]\n",
        "#             ix2 = x2[i]\n",
        "#             iy2 = y2[i]\n",
        "#             iarea = areas[i]\n",
        "\n",
        "#             for _j in range(_i + 1, num_detections):\n",
        "#                 j = order[_j]\n",
        "#                 if suppressed[j]:\n",
        "#                     continue\n",
        "\n",
        "#                 xx1 = max(ix1, x1[j])\n",
        "#                 yy1 = max(iy1, y1[j])\n",
        "#                 xx2 = min(ix2, x2[j])\n",
        "#                 yy2 = min(iy2, y2[j])\n",
        "#                 w = max(0, xx2 - xx1 + 1)\n",
        "#                 h = max(0, yy2 - yy1 + 1)\n",
        "\n",
        "#                 inter = w * h\n",
        "#                 ovr = inter / (iarea + areas[j] - inter)\n",
        "#                 if ovr >= nms_thresh:\n",
        "#                     suppressed[j] = True\n",
        "\n",
        "#             return keep\"\"\")"
      ],
      "metadata": {
        "id": "1o0jahPgxe7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read and modify centerface.py\n",
        "# with open('CenterFace/prj-python/centerface.py', 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Replace deprecated numpy types\n",
        "# content = content.replace('np.bool', 'bool')\n",
        "# content = content.replace('dtype=np.bool', 'dtype=bool')\n",
        "# content = content.replace('np.float', 'float')\n",
        "# content = content.replace('dtype=float32', 'dtype=np.float32')\n",
        "# content = content.replace('np.int', 'int')\n",
        "\n",
        "# # Write back\n",
        "# with open('CenterFace/prj-python/centerface.py', 'w') as file:\n",
        "#     file.write(content)"
      ],
      "metadata": {
        "id": "3Tpfo6OIQipY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For RetinaFace: To fix the np.float and np.int error"
      ],
      "metadata": {
        "id": "wDdiCwZlEu4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # To fix the np.float and np.int deprecation errors for retinaface\n",
        "# # You may need to restart session/kernel after doing this to fix the errors\n",
        "\n",
        "import retinaface\n",
        "print(\"RetinaFace location:\", retinaface.__file__)\n",
        "\n",
        "!sed -i 's/np.float/float/g;s/np.int/int/g' /usr/local/lib/python3.11/dist-packages/retinaface/commons/postprocess.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxqr1kknxbgn",
        "outputId": "0423ded5-df02-4aa7-9764-e40f37bbc630"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RetinaFace location: /usr/local/lib/python3.11/dist-packages/retinaface/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For YuNet"
      ],
      "metadata": {
        "id": "jHO5JG7lUOKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYhGbWw3URZK",
        "outputId": "174bff34-8861-4e03-c183-f67a9119aada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 06:02:39--  https://github.com/opencv/opencv_zoo/raw/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/opencv/opencv_zoo/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx [following]\n",
            "--2025-02-06 06:02:39--  https://media.githubusercontent.com/media/opencv/opencv_zoo/main/models/face_detection_yunet/face_detection_yunet_2023mar.onnx\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 232589 (227K) [application/octet-stream]\n",
            "Saving to: ‘face_detection_yunet_2023mar.onnx’\n",
            "\n",
            "face_detection_yune 100%[===================>] 227.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-06 06:02:40 (18.3 MB/s) - ‘face_detection_yunet_2023mar.onnx’ saved [232589/232589]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For ULFG (Ultra-Light-Fast-Generic-Face-Detector)"
      ],
      "metadata": {
        "id": "_BcowiPhYsAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj8-cDLoYz8E",
        "outputId": "eeac3355-3dba-4e03-a773-3ee3cb4c68ca"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Ultra-Light-Fast-Generic-Face-Detector-1MB'...\n",
            "remote: Enumerating objects: 953, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 953 (delta 120), reused 104 (delta 104), pack-reused 784 (from 1)\u001b[K\n",
            "Receiving objects: 100% (953/953), 37.29 MiB | 26.80 MiB/s, done.\n",
            "Resolving deltas: 100% (482/482), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/raw/master/masked_face/pretrained/RFB-640-masked_face-v2.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGJBdEaafEbS",
        "outputId": "93dd2dd0-8064-4d7a-add4-8c67a5a9240c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 22:08:03--  https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/raw/master/masked_face/pretrained/RFB-640-masked_face-v2.pth\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/master/masked_face/pretrained/RFB-640-masked_face-v2.pth [following]\n",
            "--2025-02-06 22:08:04--  https://raw.githubusercontent.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/master/masked_face/pretrained/RFB-640-masked_face-v2.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1237721 (1.2M) [application/octet-stream]\n",
            "Saving to: ‘RFB-640-masked_face-v2.pth’\n",
            "\n",
            "RFB-640-masked_face 100%[===================>]   1.18M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-06 22:08:04 (50.1 MB/s) - ‘RFB-640-masked_face-v2.pth’ saved [1237721/1237721]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add repository to path\n",
        "sys.path.append('Ultra-Light-Fast-Generic-Face-Detector-1MB')\n",
        "from vision.ssd.config.fd_config import define_img_size\n",
        "from vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor\n",
        "from vision.utils.misc import Timer"
      ],
      "metadata": {
        "id": "d0KIPH4PfXeX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For DSFD **(NOT USING ANYMORE BECAUSE OF ISSUES)**"
      ],
      "metadata": {
        "id": "n4PyHIB1Hbn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tencent/FaceDetection-DSFD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrQSLAImS7i5",
        "outputId": "c257e75a-6598-4ad3-c243-2f8124566cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FaceDetection-DSFD'...\n",
            "remote: Enumerating objects: 371, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 371 (delta 2), reused 4 (delta 1), pack-reused 364 (from 1)\u001b[K\n",
            "Receiving objects: 100% (371/371), 148.63 MiB | 15.83 MiB/s, done.\n",
            "Resolving deltas: 100% (170/170), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-\" -O WIDERFace_DSFD_RES152.pth && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# Install gdown for Google Drive downloads\n",
        "!pip install gdown\n",
        "\n",
        "# Download weight using gdown\n",
        "!gdown 1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRYWsvcBNz5S",
        "outputId": "85bdb4db-21e7-41d7-8e39-6e5f9459738f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-\n",
            "From (redirected): https://drive.google.com/uc?id=1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-&confirm=t&uuid=009addaf-e677-481a-881c-db9bbdf80fb6\n",
            "To: /content/WIDERFace_DSFD_RES152.pth\n",
            "100% 481M/481M [00:07<00:00, 65.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO FIX \"RuntimeError: Legacy autograd function with non-static forward method is deprecated\"\n",
        "\n",
        "# Then modify the specific lines\n",
        "with open('FaceDetection-DSFD/face_ssd.py', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Print the exact line we need to change to verify\n",
        "print(\"Line 345:\", lines[344])\n",
        "\n",
        "# Modify line 345 and surrounding lines\n",
        "lines[344] = '                output = self.detect.forward(\\n'\n",
        "lines[345] = '                    face_loc.view(face_loc.size(0), -1, 4),\\n'\n",
        "lines[346] = '                    self.softmax(face_conf.view(face_conf.size(0), -1, self.num_classes)),\\n'\n",
        "lines[347] = '                    self.priors.type(type(x.data))\\n'\n",
        "\n",
        "# Write back\n",
        "with open('FaceDetection-DSFD/face_ssd.py', 'w') as file:\n",
        "    file.writelines(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd0J68ZzLt3d",
        "outputId": "20db451f-2283-4e26-c8da-bcfd05cdec63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line 345:                 output = self.detect.forward(\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO REMOVE \"pa_sfd_match\"\n",
        "\n",
        "# Read the file\n",
        "with open('FaceDetection-DSFD/layers/modules/multibox_loss.py', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Remove pa_sfd_match from import\n",
        "content = content.replace(\n",
        "    'from ..box_utils import (log_sum_exp, match, pa_sfd_match, refine_match,',\n",
        "    'from ..box_utils import (log_sum_exp, match, refine_match,'\n",
        ")\n",
        "\n",
        "# Write back\n",
        "with open('FaceDetection-DSFD/layers/modules/multibox_loss.py', 'w') as file:\n",
        "    file.write(content)"
      ],
      "metadata": {
        "id": "B8BPmQtDdtYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('FaceDetection-DSFD/layers/box_utils.py', 'r') as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "# Modify the decode function\n",
        "decode_modification = \"\"\"\n",
        "def decode(loc, priors, variances):\n",
        "    # Ensure everything is on the same device\n",
        "    device = loc.device\n",
        "    priors = priors.to(device)\n",
        "    variances = [torch.tensor(v, device=device) for v in variances]\n",
        "\n",
        "    boxes = torch.cat((\n",
        "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    return boxes\n",
        "\"\"\"\n",
        "\n",
        "# Find the decode function and replace it\n",
        "with open('FaceDetection-DSFD/layers/box_utils.py', 'w') as file:\n",
        "    in_decode = False\n",
        "    for line in content:\n",
        "        if line.startswith('def decode('):\n",
        "            in_decode = True\n",
        "            file.write(decode_modification)\n",
        "        elif in_decode and line.strip() == '':\n",
        "            in_decode = False\n",
        "        elif not in_decode:\n",
        "            file.write(line)"
      ],
      "metadata": {
        "id": "HIiQ29s6aQGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('FaceDetection-DSFD')\n",
        "from face_ssd import build_ssd"
      ],
      "metadata": {
        "id": "qlPWcB9BQx8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For DBFace"
      ],
      "metadata": {
        "id": "mycjv9cYxa4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dlunion/DBFace\n",
        "!wget https://github.com/dlunion/DBFace/raw/master/model/dbface.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTq0GcPFvJdt",
        "outputId": "23686163-a329-47c7-8786-749b8bdee416"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DBFace'...\n",
            "remote: Enumerating objects: 379, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 379 (delta 46), reused 43 (delta 43), pack-reused 320 (from 1)\u001b[K\n",
            "Receiving objects: 100% (379/379), 50.43 MiB | 24.02 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n",
            "--2025-02-06 19:22:54--  https://github.com/dlunion/DBFace/raw/master/model/dbface.pth\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dlunion/DBFace/master/model/dbface.pth [following]\n",
            "--2025-02-06 19:22:55--  https://raw.githubusercontent.com/dlunion/DBFace/master/model/dbface.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7372999 (7.0M) [application/octet-stream]\n",
            "Saving to: ‘dbface.pth’\n",
            "\n",
            "dbface.pth          100%[===================>]   7.03M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-06 19:22:55 (147 MB/s) - ‘dbface.pth’ saved [7372999/7372999]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('DBFace')\n",
        "from model.DBFace import DBFace\n",
        "import common"
      ],
      "metadata": {
        "id": "hZpTrrk5wRff"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating folder to store processed videos"
      ],
      "metadata": {
        "id": "RpBcBkAlfnjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory if it doesn't exist\n",
        "output_dir = '/content/processed_videos'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "metadata": {
        "id": "nJRsJNnde5dV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "dyOmkF7VgOow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "S2avE-DSQJfb"
      },
      "outputs": [],
      "source": [
        "def test_single_model(video_path, model_name):\n",
        "    # Initialize chosen model\n",
        "    if model_name == \"mediapipe\":\n",
        "        mp_face_detection = mp.solutions.face_detection\n",
        "        model = mp_face_detection.FaceDetection(\n",
        "            model_selection=1,\n",
        "            min_detection_confidence=0.55\n",
        "        )\n",
        "\n",
        "    elif model_name == \"mtcnn\":\n",
        "        model = MTCNN()\n",
        "\n",
        "    elif model_name == \"retinaface\":\n",
        "        pass\n",
        "\n",
        "    elif model_name == \"yolov11\":\n",
        "        # HAVE TO MANUALLY DOWNLOAD IT FROM GITHUB README (https://github.com/akanametov/yolo-face) THEN UPLOAD IT TO COLAB\n",
        "        model = YOLO('yolov11l-face.pt')\n",
        "\n",
        "    elif model_name == \"insightface\":\n",
        "        # Check if GPU is available\n",
        "        ctx_id = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n",
        "        model = insightface.app.FaceAnalysis()\n",
        "        model.prepare(ctx_id=ctx_id)\n",
        "\n",
        "    elif model_name == \"opencv-dnn\":\n",
        "        # Define model files\n",
        "        model_file = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "        config_file = \"deploy.prototxt\"\n",
        "\n",
        "        # Download model files if they don't exist\n",
        "        if not os.path.exists(model_file):\n",
        "            print(\"Downloading model file...\")\n",
        "            !wget https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel -O res10_300x300_ssd_iter_140000.caffemodel\n",
        "\n",
        "        if not os.path.exists(config_file):\n",
        "            print(\"Downloading config file...\")\n",
        "            !wget https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt -O deploy.prototxt\n",
        "\n",
        "        # Verify files exist\n",
        "        if not os.path.exists(model_file) or not os.path.exists(config_file):\n",
        "            raise FileNotFoundError(f\"Required files not found: {model_file} or {config_file}\")\n",
        "\n",
        "        model = cv2.dnn.readNet(model_file, config_file)\n",
        "\n",
        "\n",
        "    elif model_name == \"dlib\":\n",
        "        # Load Dlib's face detector\n",
        "        model = dlib.get_frontal_face_detector()\n",
        "\n",
        "\n",
        "    elif model_name == \"yunet\":\n",
        "        # Initialize YuNet\n",
        "        model = cv2.FaceDetectorYN.create(\n",
        "            model=\"face_detection_yunet_2023mar.onnx\",\n",
        "            config=\"\",\n",
        "            input_size=(640, 480),  # Can adjust based on my needs\n",
        "            score_threshold=0.6,\n",
        "            nms_threshold=0.3,\n",
        "            top_k=50,\n",
        "            backend_id=cv2.dnn.DNN_BACKEND_DEFAULT,\n",
        "            target_id=cv2.dnn.DNN_TARGET_CPU\n",
        "        )\n",
        "\n",
        "    elif model_name == \"ulfg\":\n",
        "        # Set device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        # Set image size\n",
        "        define_img_size(640)\n",
        "\n",
        "        # Create model with matching dimensions\n",
        "        net = create_Mb_Tiny_RFB_fd(3, is_test=True, device=device)\n",
        "\n",
        "        try:\n",
        "            # Load the weights\n",
        "            checkpoint = torch.load(\"RFB-640-masked_face-v2.pth\", map_location=device)\n",
        "            net.load_state_dict(checkpoint)\n",
        "            # Create predictor with candidate_size here\n",
        "            predictor = create_Mb_Tiny_RFB_fd_predictor(net,\n",
        "                                                      candidate_size=1500,\n",
        "                                                      device=device)\n",
        "            print(\"Model loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            raise e\n",
        "\n",
        "\n",
        "    elif model_name == \"dbface\":\n",
        "        # Initialize model\n",
        "        model = DBFace()\n",
        "        model.eval()\n",
        "        model.load(\"dbface.pth\")  # Use regular model, not small\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "        # Define preprocessing parameters\n",
        "        mean = [0.408, 0.447, 0.47]\n",
        "        std = [0.289, 0.274, 0.278]\n",
        "\n",
        "\n",
        "    # elif model_name == \"centerface\":\n",
        "    #   centerface = CenterFace(landmarks=False)\n",
        "    #   layer_names = centerface.net.getLayerNames()\n",
        "    #   print(\"Available layers:\", layer_names)\n",
        "    #   model = centerface\n",
        "\n",
        "\n",
        "    # elif model_name == \"dsfd\":\n",
        "    #     # Initialize model\n",
        "    #     net = build_ssd('test')\n",
        "\n",
        "    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #     print(f\"Using device: {device}\")\n",
        "\n",
        "    #     # Load weights and verify\n",
        "    #     weights = torch.load('WIDERFace_DSFD_RES152.pth', map_location=device)\n",
        "    #     print(\"Weight keys:\", weights.keys())  # Debug weights\n",
        "\n",
        "    #     net = net.to(device)\n",
        "    #     net.load_state_dict(weights)\n",
        "    #     net.eval()\n",
        "\n",
        "    #     if hasattr(net, 'detect'):\n",
        "    #         net.detect.variance = [torch.tensor(v, device=device) for v in net.detect.variance]\n",
        "\n",
        "    #     if hasattr(net, 'priors'):\n",
        "    #         net.priors = net.priors.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # Opening input video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Getting video properties\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Calculating output dimensions (70% of original)\n",
        "    output_width = int(width * 0.7)\n",
        "    output_height = int(height * 0.7)\n",
        "\n",
        "    # Create output video writer\n",
        "    '''\n",
        "    The processed mp4 file sizes were too large for my liking, so I wanted to compress it using H.264 codec.\n",
        "    Problem was, the processed vids were now corrupt or some shet. Wasn't letting me play them locally.\n",
        "    So, decided to go with .avi and XVID codec as it's more reliable.\n",
        "    The issue with MP4 in OpenCV is that it sometimes creates files that aren't properly finalized/encoded, making them unplayable.\n",
        "    AVI with XVID is more consistently supported.\n",
        "    '''\n",
        "    output_path = os.path.join(output_dir, f'processed_{model_name}.avi')\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_path,\n",
        "                         fourcc,\n",
        "                         fps,\n",
        "                         (output_width, output_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize frame once to output dimensions\n",
        "        frame = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_LANCZOS4)\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get detections based on model type\n",
        "        if model_name == \"mediapipe\":\n",
        "            results = model.process(rgb_frame)\n",
        "            if results.detections:\n",
        "                for detection in results.detections:\n",
        "                    bbox = detection.location_data.relative_bounding_box\n",
        "                    h, w, _ = frame.shape\n",
        "                    x = int(bbox.xmin * w)\n",
        "                    y = int(bbox.ymin * h)\n",
        "                    w = int(bbox.width * w)\n",
        "                    h = int(bbox.height * h)\n",
        "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        elif model_name == \"mtcnn\":\n",
        "            results = model.detect_faces(rgb_frame)\n",
        "            for result in results:\n",
        "                x, y, w, h = result['box']\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "        elif model_name == \"retinaface\":\n",
        "            results = RetinaFace.detect_faces(frame)\n",
        "            if isinstance(results, dict):\n",
        "                for key in results.keys():\n",
        "                    face = results[key]\n",
        "                    facial_area = face['facial_area']\n",
        "                    cv2.rectangle(frame,\n",
        "                                (facial_area[0], facial_area[1]),\n",
        "                                (facial_area[2], facial_area[3]),\n",
        "                                (0, 255, 0),\n",
        "                                2)\n",
        "\n",
        "        elif model_name == \"yolov11\":\n",
        "          results = model(frame)[0]\n",
        "          for result in results.boxes.data:\n",
        "              x1, y1, x2, y2, conf, _ = result\n",
        "              if conf > 0.55:  # Confidence threshold\n",
        "                  cv2.rectangle(frame,\n",
        "                              (int(x1), int(y1)),\n",
        "                              (int(x2), int(y2)),\n",
        "                              (0, 255, 0),\n",
        "                              2)\n",
        "\n",
        "        elif model_name == \"insightface\":\n",
        "            faces = model.get(rgb_frame)\n",
        "            for face in faces:\n",
        "                bbox = face.bbox.astype(int)\n",
        "                cv2.rectangle(frame,\n",
        "                            (bbox[0], bbox[1]),\n",
        "                            (bbox[2], bbox[3]),\n",
        "                            (0, 255, 0),\n",
        "                            2)\n",
        "\n",
        "        elif model_name == \"opencv-dnn\":\n",
        "            blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123])\n",
        "            model.setInput(blob)\n",
        "            detections = model.forward()\n",
        "            for i in range(detections.shape[2]):\n",
        "                confidence = detections[0, 0, i, 2]\n",
        "                if confidence > 0.5:\n",
        "                    box = detections[0, 0, i, 3:7] * np.array([output_width, output_height, output_width, output_height])\n",
        "                    (x1, y1, x2, y2) = box.astype(\"int\")\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        elif model_name == \"dlib\":\n",
        "            # Dlib works with grayscale images\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            faces = model(gray)\n",
        "            for face in faces:\n",
        "                x = face.left()\n",
        "                y = face.top()\n",
        "                w = face.right() - face.left()\n",
        "                h = face.bottom() - face.top()\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        elif model_name == \"yunet\":\n",
        "            # Set input size\n",
        "            model.setInputSize((frame.shape[1], frame.shape[0]))\n",
        "\n",
        "            # Detect faces\n",
        "            _, faces = model.detect(frame)\n",
        "\n",
        "            # Draw detections\n",
        "            if faces is not None:\n",
        "                for face in faces:\n",
        "                    box = list(map(int, face[:4]))\n",
        "                    # Draw rectangle\n",
        "                    cv2.rectangle(frame,\n",
        "                                (box[0], box[1]),\n",
        "                                (box[0] + box[2], box[1] + box[3]),\n",
        "                                (0, 255, 0),\n",
        "                                2)\n",
        "\n",
        "\n",
        "        elif model_name == \"ulfg\":\n",
        "            # Convert to RGB for model\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Remove candidate_size from predict call\n",
        "            boxes, labels, probs = predictor.predict(image,\n",
        "                                                   prob_threshold=0.4)  # Just use threshold\n",
        "\n",
        "            # Process detections\n",
        "            for i in range(boxes.size(0)):\n",
        "                box = boxes[i, :]\n",
        "                prob = probs[i]\n",
        "\n",
        "                if prob > 0.4:  # Confidence threshold\n",
        "                    x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
        "\n",
        "                    # Draw detection\n",
        "                    cv2.rectangle(frame,\n",
        "                                (x1, y1),\n",
        "                                (x2, y2),\n",
        "                                (0, 255, 0),\n",
        "                                2)\n",
        "\n",
        "                    # Add label\n",
        "                    label = f\"{prob:.2f}\"\n",
        "                    cv2.putText(frame,\n",
        "                              label,\n",
        "                              (x1, y1 - 10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                              0.5,\n",
        "                              (0, 255, 0),\n",
        "                              2)\n",
        "\n",
        "\n",
        "        elif model_name == \"dbface\":\n",
        "            # Preprocess frame\n",
        "            image = common.pad(frame)\n",
        "            image = ((image / 255.0 - mean) / std).astype(np.float32)\n",
        "            image = image.transpose(2, 0, 1)\n",
        "\n",
        "            # Convert to tensor\n",
        "            torch_image = torch.from_numpy(image)[None]\n",
        "\n",
        "            # Get detections\n",
        "            with torch.no_grad():\n",
        "                hm, box, landmark = model(torch_image)\n",
        "\n",
        "                # Post-process\n",
        "                hm_pool = F.max_pool2d(hm, 3, 1, 1)\n",
        "                scores, indices = ((hm == hm_pool).float() * hm).view(1, -1).cpu().topk(1000)\n",
        "\n",
        "                scores = scores.squeeze()\n",
        "                indices = indices.squeeze()\n",
        "                hm_height, hm_width = hm.shape[2:]\n",
        "\n",
        "                ys = list((indices / hm_width).int().data.numpy())\n",
        "                xs = list((indices % hm_width).int().data.numpy())\n",
        "                scores = list(scores.data.numpy())\n",
        "                box = box.cpu().squeeze().data.numpy()\n",
        "\n",
        "                # Process detections\n",
        "                stride = 4\n",
        "                threshold = 0.4\n",
        "                for cx, cy, score in zip(xs, ys, scores):\n",
        "                    if score < threshold:\n",
        "                        break\n",
        "\n",
        "                    # Get bounding box\n",
        "                    x, y, r, b = box[:, cy, cx]\n",
        "                    xyrb = (np.array([cx, cy, cx, cy]) + [-x, -y, r, b]) * stride\n",
        "\n",
        "                    # Draw detection\n",
        "                    x1, y1, x2, y2 = map(int, xyrb)\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, f\"{score:.2f}\",\n",
        "                              (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                              0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "\n",
        "        # elif model_name == \"centerface\":\n",
        "        #   h, w = frame.shape[:2]\n",
        "        #   dets, _ = model(frame, h, w)\n",
        "        #   for det in dets:\n",
        "        #       boxes, score = det[:4], det[4]\n",
        "        #       if score < 0.5:  # confidence threshold\n",
        "        #           continue\n",
        "        #       x1, y1, x2, y2 = boxes.astype(int)\n",
        "        #       cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        # elif model_name == \"dsfd\":\n",
        "        #     # Enhanced preprocessing\n",
        "        #     img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        #     img = torch.from_numpy(img).float().permute(2, 0, 1)\n",
        "        #     # Normalize properly\n",
        "        #     img = (img - 128) / 128.0  # Try different normalization\n",
        "        #     img = img.unsqueeze(0).to(device)\n",
        "\n",
        "        #     try:\n",
        "        #         with torch.no_grad():\n",
        "        #             detections = net(img)\n",
        "\n",
        "        #         # Print raw detection stats\n",
        "        #         print(f\"Detection stats - Min: {detections.min():.3f}, Max: {detections.max():.3f}\")\n",
        "\n",
        "        #         # Move to CPU and convert to numpy\n",
        "        #         detections = detections.cpu().numpy()\n",
        "\n",
        "        #         # Handle NMS error\n",
        "        #         if detections.shape[2] > 0:  # If we have any detections\n",
        "        #             for i in range(detections.shape[2]):\n",
        "        #                 confidence = detections[0, 0, i, 0]\n",
        "        #                 if confidence > 0.1:  # Lower threshold for testing\n",
        "        #                     box = detections[0, 0, i, 1:5] * np.array([width, height, width, height])\n",
        "        #                     (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        #                     cv2.rectangle(frame,\n",
        "        #                                 (startX, startY),\n",
        "        #                                 (endX, endY),\n",
        "        #                                 (0, 255, 0),\n",
        "        #                                 2)\n",
        "        #                     # Print successful detection\n",
        "        #                     print(f\"Detection found! Confidence: {confidence:.3f}\")\n",
        "\n",
        "        #     except Exception as e:\n",
        "        #         print(f\"Error processing frame: {str(e)}\")\n",
        "        #         continue  # Skip problematic frames\n",
        "\n",
        "\n",
        "\n",
        "        # Write frame\n",
        "        out.write(frame)\n",
        "\n",
        "        # Print progress\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Processing {model_name}: {frame_count}/{total_frames} frames\")\n",
        "\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Infer"
      ],
      "metadata": {
        "id": "zs9UZ9lkfcVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload video\n",
        "print(\"Please upload your video file\")\n",
        "uploaded = files.upload()\n",
        "video_path = next(iter(uploaded))\n",
        "\n",
        "# Test each model\n",
        "# \"mediapipe\", \"mtcnn\", \"retinaface\", \"yolov11\", \"insightface\", \"opencv-dnn\", \"dlib\", \"yunet\", \"ulfg\", \"dbface\"\n",
        "for model_name in [\"retinaface\"]:\n",
        "    print(f\"\\nStarting {model_name}...\")\n",
        "    test_single_model(video_path, model_name)\n",
        "\n",
        "print(\"\\nAll processing complete. Videos are saved in /content/processed_videos/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Cbv9wj7hfZlO",
        "outputId": "32a70b3c-be08-4381-90ed-b99440801aae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your video file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-414cd15f-33e2-4c96-bbee-62f9624bf30d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-414cd15f-33e2-4c96-bbee-62f9624bf30d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving d4_c3.mp4 to d4_c3 (1).mp4\n",
            "\n",
            "Starting retinaface...\n",
            "Directory  /root /.deepface created\n",
            "Directory  /root /.deepface/weights created\n",
            "retinaface.h5 will be downloaded from the url https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n",
            "To: /root/.deepface/weights/retinaface.h5\n",
            "100%|██████████| 119M/119M [00:00<00:00, 208MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing retinaface: 30/122 frames\n",
            "Processing retinaface: 60/122 frames\n",
            "Processing retinaface: 90/122 frames\n",
            "Processing retinaface: 120/122 frames\n",
            "\n",
            "All processing complete. Videos are saved in /content/processed_videos/\n"
          ]
        }
      ]
    }
  ]
}